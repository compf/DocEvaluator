\section{Metriken}\label{chapter:metrics}
Im Folgenden werden die Metriken präsentiert, die in der finalen Version des \textit{JavadocEvaluator} implementiert worden sind. Dabei werden pro Metrik einige Vor- und Nachteile genannt, um den zukünftigen Anwender eine Entscheidungshilfe zu bieten. 
\subsection{Metrik: Anteil dokumentierter Komponenten an allen Komponenten}
Bei dieser Metrik werden dokumentierte Komponenten mit hundert Punkten bewertet, während undokumentierte Komponenten null Punkte erhalten. Dabei ist es irrelevant, wie gut die Dokumentation ist. Selbst die leere Dokumentation \textbf{/***/} würde ein Rating von 100 erhalten, da hier eine weitergehende Differenzierung zwar umsetzbar ist, aber dafür eine davon abgeleitete Metrik besser geeignet wäre. Vorteilhaft an dieser Metrik ist die Einfachheit der Implementierung. Außerdem ist sie als grober Indikator gut geeignet, die Qualität der Dokumentation abzuschätzen. Ein Nachteil an dieser Metrik ist, dass sie auch sinnlose Kommentare, die leer oder nichts mit der dokumentierten Komponente zu tun haben, mit der vollen Punktzahl bewertet werden. Außerdem bestehen moderne Softwareprojekte aus vielen verschiedenen Komponenten, wobei einige Komponenten nur an einigen wenigen Stellen verwendet werden und daher nur selten dokumentiert wurden. Aus diesen Grund würde diese Metrik die \enquote{wahre} Dokumentationsqualität unterschätzen. 
Diese Metrik wurde unter anderem in \cite[S. 5]{HowDocumentationEvolvesoverTime} erläutert und dort als \enquote{ANYJ} bezeichnet. Da dies einfache Metrik ist und für eine grobe Einschätzung der Qualität der Dokumentation geeignet ist, wurde diese Metrik im Tool implementiert.

\subsection{Metrik: Anteil dokumentierter öffentlicher Komponenten an allen öffentlichen Komponenten }
Diese Metrik basiert auf der vorherigen Metrik und verwendet sie sogar intern. Im Unterschied zu dieser werden hier nur öffentliche Komponenten betrachtet. Dies bedeutet, dass alle nicht öffentlichen Komponenten so betrachtet werden, als wären sie nicht existent. Die öffentlichen Komponenten dagegen werden wie oben mit hundert bzw. null Punkten bewertet. Dies hat den Vorteil, dass nur die Komponenten, die wahrscheinlich von anderen Komponenten verwendet werden müssen und daher auch gut verstanden werden müssen, bei der Bewertung der Qualität der Dokumentation berücksichtigt werden. So hat der Entwickler einen guten Überblick über die Stellen, die nach gängiger Erfahrung unbedingt besser dokumentiert werden müssen. Auf der anderen Seite werden sinnlose Kommentare weiterhin akzeptiert und nicht öffentliche Komponenten werden ohne Berücksichtigung ihrer Wichtigkeit ignoriert, obwohl auch hier eine gute Dokumentation Fehler vermeiden kann und zu einer besseren Softwarequalität führen kann. 

Diese Metrik wurde unter anderem von \cite{Doautomaticrefactoringsimprovemaintainability?Anindustrialcasestudy} verwendet, um die Wirkung von Refactorings zu analysieren, Auch PMD und Checkstyle besitzen Optionen, um nur öffentliche Komponenten zu analysieren, sodass diese Metrik durchaus Relevanz hat und daher implementiert wurde.

 
 \subsection{Metrik: Vollständigkeit der Dokumentation von  Methoden}
 
Viele Methoden in objektorientierten Programmiersprachen besitzen Parameter und haben einen Rückgabewert. Diese stellen die Eingabe und Ausgabe der Methode dar und müssen genau verstanden werden, um sie korrekt anzuwenden. Dazu ist eine gute Dokumentation sinnvoll. In Javadoc und vielen anderen Dokumentationssystemen können einzelne Parameter dokumentiert werden. Zur Bewertung der Dokumentation kann geprüft werden, wie viele Parameter der Methode dokumentiert sind. Demgegenüber können auch Parameter dokumentiert werden, die gar nicht mehr existieren. Dies kann beispielsweise das Resultat von einem Refactoring sein. Diese \enquote{Phantomparameter} können verwirren und zu Problemen führen, sodass es auch hier sinnvoll ist, die fehlerhaft dokumentierten Parameter gezählt werden können. In dieser Metrik jeweils der Anteil der falsch dokumentierten Parameter (seien sie existent aber nicht dokumentiert oder dokumentiert aber nicht existent) berechnet und so eine Bewertung ermittelt.
 
 Dazu sollte auch der Rückgabewert überprüft werden. Eine Methode ohne Dokumentation des Rückgabewertes wird mit null Punkten bewertet, während eine Methode mit dokumentierten Rückgabewert mit hundert Punkten bewertet wird. Natürlich werden dabei Methoden, die \enquote{void} zurückgeben oder Konstruktoren auch mit hundert Punkten bewertet, da diese keinen dokumentierbaren Rückgabewert besitzen. 
 
 Diese Metrik wird auch in Checkstyle implementiert, das auf nicht dokumentierte Parameter hinweist. In der wissenschaftlichen Literatur wird diese Metrik unter anderem in \cite[S. 5]{HowDocumentationEvolvesoverTime} genannt und dort als \enquote{DIR} bezeichnet. Da eine vollständig dokumentierte Methode zu einer guten Dokumentationsqualität beiträgt, wurde auch diese Metrik aufgenommen.

\subsection{Metrik: Ignorieren von Getter und Setter }
Getter und Setter sind typische Konstrukte in objektorientierten Programmiersprachen, die es ermöglichen, auf ein gekapseltes Feld zu lesen oder es zu verändern. Dadurch ermöglichen sie einen kontrollierten Zugriff auf die Felder und können so Fehler vermeiden \cite[S. 235]{OntheUseofPropertiesinJavaApplications}. Viele dieser Getter- und Setter-Methoden werden oft nicht dokumentiert, da sie nur triviale Codezeilen enthalten und sehr kurz sind, sodass eine Dokumentierung den Code aufbläht und keine Vorteile bietet. \cite[S. 254]{JavadocViolationsandTheirEvolutioninOpen-SourceSoftware}. Auch Checkstyle bietet die Option an, diese trivialen Methoden bei der Analyse der Softwaredokumentation auszuschließen. 

Nicht trivial ist dagegen die Implementierung einer Metrik, die Getter- und Setter-Methoden ausschließt. Laut der JavaBeans-Spezifikation \cite[S. 55]{javabeans} sollen diese Zugriffsmethoden mit dem Präfix \enquote{get} bzw. \enquote{set} beginnen. Außerdem besitzt ein Getter keine Parameter und besitzt einen Rückgabetyp. Ein Setter sollte den Rückgabetyp \enquote{void} haben und exakt ein Parameter übergeben bekommen. Abweichend davon können Getter, die einen Boolean zurückgeben, mit dem Präfix \enquote{is} beginnen. Die Namenskonvention ist allerdings sprachspezifisch und noch kein eindeutiges Indiz für einen Getter bzw. Setter. Nichtsdestotrotz wurde entschieden die Filterung nach den JavaBeans-Kriterien durchzuführen und dafür eine Möglichkeit anzubieten, die Namenskonvention nach eigenen Wünschen zu konfigurieren. 

Der Vorteil des Ausschließens von Getter und Setter wurde bereits genannt. Die Kommentare von diesen Zugriffsmethoden sind oft repetitiv und enthalten keinen Mehrwert, sodass sie überlesen werden und der größere Code sogar unübersichtlicher wird. Allerdings sollte die dazugehörigen Felder trotzdem dokumentiert werden, um ihren Sinn und Zweck zu begründen. Dies wird hier nicht durchgesetzt, sodass es dazu kommen kann, dass Felder nicht dokumentiert sind und auch die dazugehörigen Zugriffsmethoden keine Hilfe bieten. Dies führt vermutlich zu einer schlechteren Lesbarkeit, da jedes Feld eine Bedeutung hat, die nicht immer offensichtlich ist.
\subsection{Flesch-Reading Ease Score}
Der Flesch-Reading-Ease-Score\cite[S. 21]{ThePrinciplesofReadability} ist eine einfache Formel zur heuristischen Ermittlung der Lesbarkeit eines englischsprachigen Textes. Dabei werden die Anzahl der Sätze, die Anzahl der Wörter und die Anzahl der Silben im Text als Eingabeparameter. Die Formel lautet $206.835-1.015*(W/S)-84.6*(H/W)$. Dabei ist $S$ die Anzahl der Sätze, $W$ die Anzahl der Wörter und $H$ die Anzahl der Silben. 

Die Formel liefert normalerweise einen Wert von 0 bis 100 zurück. Dabei bedeutet der Wert 0, dass der Text sehr kompliziert geschrieben ist und nur von Personen mit akademischer Bildung gut verstanden werden kann. Ein Wert von 100 bedeutet dagegen, dass der Text sehr einfach aufgebaut ist und daher schon von einem Fünftklässler verstanden werden kann. Diese Formel und verwandte Formeln werden auch von verschiedenen US-Behörden verwendet, um die Lesbarkeit ihrer Dokumente zu verbessern\cite[S. 72]{AutomaticQualityAssessmentofSourceCodeComments:TheJavadocMiner}

Die Flesch-Metrik nutzt diese Formeln, um eine Bewertung der Softwaredokumentation zu erhalten. Dabei wird angenommen, dass ein Flesch-Score von ca. 70 optimal ist. Dokumente mit einer Bewertung von 60 - 70 werden von etwas mehr als 80 Prozent der erwachsenen US-Bevölkerung verstanden, sodass diese Texte lesbar genug sein sollten.

Aus diesen Grund wurde eine mathematische Funktion ermittelt, die für einen Flesch-Score von 70 eine Bewertung von 100 zurückgibt. Für einen größeren Flesch-Score wird eine lineare Funktion verwendet, die für einen sehr leichten Text (Flesh-Score 100 oder darüber) eine Bewertung von 85 zurückgibt. Damit sollen Programmierer für sehr leichte Texte nicht stark bestraft, aber trotzdem ermuntert werden einen für technische Dokumentationen angemessenen Text zu schreiben.

Für einen Flesch-Score unter 70 wird eine quadratische Funktion verwendet, die einen Flesch-Score von 60 noch sehr fair behandelt, aber kleinere Flesch-Scores, die auf einen komplizierten Text hindeuten, deutlich schlechter bewertet werden. Damit sollen Entwickler dazu ermuntert werden, ihre Texte nicht allzu kompliziert zu gestalten. Dies hilft nachfolgenden Entwicklern, aber auch dem Erzeuger der Dokumentation, beim Verständnis des Quellcodes und verhindert so potenzielle Bugs.

Bei der Verwendung des Flesch-Scores sollte beachtet werden, dass die Bewertung von Flesch bei kurzen Texten, zum Beispiel aus nur einem Satz, stark fluktuiert. Außerdem ist die Bestimmung der Silbenzahl eines Wortes nicht immer trivial; bei der Entwicklung des Tools musste bei einem Austausch einer Bibliothekt die Testmethode angepasst werden, da das Wort \enquote{themselves} plötzlich drei statt zwei Silben hatte. Dies hängt natürlich auch von der Aussprache und somit von kulturellen Gegebenheiten ab. Dies gilt natürlich auch für Wörter und Sätze. 
\subsubsection{Parameter}
\begin{itemize}
    \item \textit{considerTags} Wenn true, berechne den Mittelwert aus allen Flesch-Score der einzelnen Tags und der allgemeinen Beschreibung des Javadocs, standardmäßig false
\end{itemize}

 \section{Eigene Metriken}
 Im Folgenden werden Metriken aufgegriffen, die kaum in der wissenschaftlichen Literatur erwähnt werden, aber durchaus interessant sein dürften.
 \subsection{Metrik: Bestrafung von undokumentierten langen Methoden}
Es wird grundsätzlich empfohlen, möglichst kurze Methoden zu schreiben und längere Methoden in mehrere kleinere Methoden aufzuspalten. Jede Methode sollte dabei nicht mehr als vier Codezeilen lang sein \cite[S. 34]{martin2009clean}.
 
 Dennoch wird diese Empfehlung nicht immer befolgt. Im Kontext der Softwaredokumentation sollten diese größeren Methoden allerdings umso eher dokumentiert werden, da es ansonsten schwieriger ist, die Bedeutung der verschiedenen Parameter, des Rückgabewertes und allgemeinen Rolle der Funktion zu erfassen. Daher macht es Sinn undokumentierte Methoden mit vielen Codezeilen schlechter zu bewerten.
 
 Genau dafür wurde diese Metrik entwickelt. Jede Methode enthält unstrukturiert alle Befehle, Verzweigungen etc. Durch Splitten am Zeilenende lässt sich eine grobe Schätzung für die Anzahl der Codezeilen finden. Anschließend wird diese Information  an eine Funktion $B(l))S-(S-B_0)*e^(-k*l)$ übergeben, die eine beschränkt wachsende Exponentialfunktion ist. Dabei ist $S$ die untere Schranke für die Bewertung, $B_0$ die beste Bewertung und $k$ eine Konstante für die Wachstumsrate. Um lange undokumentierte Methoden besser bestrafen zu können, wird eine Fallunterscheidung vorgenommen.
 
 Bei kurzen Methoden unter zehn Zeilen beträgt die untere Schranke 90, da kurze Methoden nicht allzu schlecht bewertet werden sollen. Für Methoden mit mindestens zehn Zeilen sollte eine strengere Bewertung existieren, da diese wahrscheinlich unübersichtlicher sind. Aus diesem Grund werden längere Methoden mit nicht mehr als 90 Punkten und minimal null Punkten bewertet, sodass die Spannweite größer ist und die Bewertung schneller gegen null tendiert.
 
 Die Konstante $k$ kann vom Benutzer der Metrik frei ermittelt werden; ein Wert von $k=0.2$ scheint jedoch passend zu sein, da Methoden mit bis zu zehn Zeilen noch relativ gut bewertet werden und längere Methoden gut bestraft werden.
 
 Diese Metrik wird allerdings nur auf undokumentierte Methoden angewendet, sodass dokumentierte Methoden wie bei der Metrik \textit{SimpleCommentPresentMetric} mit 100 Punkten bewertet werden. Andere Komponenten wie zum Beispiel Klassen werden hier vollständig ignoriert, sodass sie keinen Einfluss auf das Gesamtergebnis haben.
 \subsubsection{Parameter}
 \begin{itemize}
     \item \textit{k} Die Wachstumsrate des beschränktes Wachstum
     \item \textit{ignoreLines} Zeilen, die bei der Berechnung der \ac{LOC} ignoriert werden sollen
 \end{itemize}
     

 \subsection{Anteil dokumentierter Methoden an allen Methoden unter Berücksichtigung der LOC}
Diese Metrik ist relativ ähnlich zu der vorherigen Metrik. Allerdings arbeitet diese Metrik auf dem Level von hierarchischen Komponenten (z. B. Klassen) und summiert die \ac{LOC} der dokumentierten und undokumentierten Methoden und ermittelt daraus den Anteil der dokumentierten \ac{LOC} an allen \ac{LOC}. Dadurch werden längere Methoden wichtiger als kürzere, wodurch undokumentierter Getter- oder Setter-Methoden nicht so negativ bewertet werden. In Unterschied zu der vorherigen Metrik werden auch kommentierte Methoden nach ihrer Länge bewertet, anstatt wie bei der vorherigen Metrik stets mit 100 Prozent. Dies hat den Vorteil, dass jede dokumentierte Methode anhand ihres Gewichts in puncto \ac{LOC} gewichtet wird und der Entwickler für viele dokumentierte Methoden, die in vielen Fällen mehrere Codezeilen lang sind, eher belohnt wird als bei der vorherigen Metrik. Allerdings sollte auch erwähnt werden, dass hierdurch lange Methoden, auch wenn diese kommentiert sind, positiv bewertet werden, was eindeutig gegen die Grundsätze aus \cite[S. 34]{martin2009clean} verstößt. Zudem sollte es klar sein, dass eine undokumentierte lange Methode wahrscheinlich unverständlicher ist als eine gleich lange dokumentierte Methode. Da diese beiden Methoden sich dann bei dieser Metrik ausgleichen, würde die Qualität der Softwaredokumentation nur unzureichend widergespiegelt werden.
  \subsubsection{Parameter}
  \begin{itemize}
     \item \textit{ignoreLines} Zeilen, die bei der Berechnung der \ac{LOC} ignoriert werden sollen
 \end{itemize}
 